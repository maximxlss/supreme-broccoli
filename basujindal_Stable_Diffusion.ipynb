{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maximxlss/supreme-broccoli/blob/master/basujindal_Stable_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stable Diffusion Lite by [FutureArt](https://twitter.com/future__art) and [Pharmapsychotic](https://twitter.com/pharmapsychotic)\n",
        "\n",
        "Generate images with CompVis/Stability [Stable Diffusion](https://github.com/CompVis/stable-diffusion) with bonus [KLMS sampling](https://github.com/crowsonkb/k-diffusion.git) from [@RiversHaveWings](https://twitter.com/RiversHaveWings).\n",
        "\n",
        "This notebook is a pretty faithful copy of [pharmapsychotic's Stable Diffusion notebook](https://colab.research.google.com/github/pharmapsychotic/ai-notebooks/blob/main/pharmapsychotic_Stable_Diffusion.ipynb). I just simplified some settings, wrote some instructions, and added the ability to queue up multiple prompts and run variations on the same seed.\n",
        "\n",
        "If you want more control over your settings, such as selecting what sampler to use, then I suggest using the original notebook.\n",
        "\n",
        "I plan to make improvements to this notebook. Please [follow me on Twitter](https://twitter.com/future__art) or [watch my github](https://github.com/vincefav/stable-diffusion-lite) to get updates.\n",
        "\n",
        "### How to download the Stable Diffusion model\n",
        "\n",
        "- Visit https://huggingface.co/CompVis/stable-diffusion-v-1-4-original and agree to the terms and conditions.\n",
        "- Click the **Files and versions** tab\n",
        "- Click **stable-diffusion-v-1-4-original**\n",
        "- Click the **download** link where it says *This file is stored with Git LFS . It is too big to display, but you can still download it.*\n",
        "- If you have [Google Drive for desktop](https://www.google.com/drive/download/) (highly recommended), you can save it directly to your **AI/models** directory.\n",
        "  - Otherwise, download it and re-upload it to your [Google Drive](https://drive.google.com) in the **AI/models** directory. (This is risky, as the upload may time out.)"
      ],
      "metadata": {
        "id": "UU52ZvES6-1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uc5OwvKdjRJF",
        "outputId": "53c0772c-5e97-4b68-987d-aa56647a3ccd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-3ee7383b-5a69-da00-1a1c-5994e53e0697)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup"
      ],
      "metadata": {
        "id": "0GOr30k9Gh1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Google Drive and Prepare Folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "outputs_path = \"/content/gdrive/MyDrive/AI/Stable Diffusion/images_out\"\n",
        "!mkdir -p $outputs_path\n",
        "print(f\"Outputs will be saved to {outputs_path}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DeqQ7pt1zdI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installation\n",
        "!pip install pytorch-lightning torch-fidelity\n",
        "!pip install numpy omegaconf einops kornia pytorch-lightning\n",
        "!pip install albumentations transformers\n",
        "!pip install ftfy jsonmerge resize-right torchdiffeq tqdm\n",
        "!pip install huggingface_hub gradio\n",
        "\n",
        "!git clone https://github.com/basujindal/stable-diffusion\n",
        "%cd stable-diffusion/\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/crowsonkb/k-diffusion.git\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"./CLIP\")\n",
        "sys.path.append('./taming-transformers')\n",
        "sys.path.append('./k-diffusion')\n",
        "\n",
        "!echo '' > ./k-diffusion/k_diffusion/__init__.py\n",
        "\n",
        "def split_weighted_subprompts(text):\n",
        "    \"\"\"\n",
        "    grabs all text up to the first occurrence of ':' \n",
        "    uses the grabbed text as a sub-prompt, and takes the value following ':' as weight\n",
        "    if ':' has no value defined, defaults to 1.0\n",
        "    repeats until no text remaining\n",
        "    \"\"\"\n",
        "    remaining = len(text)\n",
        "    prompts = []\n",
        "    weights = []\n",
        "    while remaining > 0:\n",
        "        if \":\" in text:\n",
        "            idx = text.index(\":\") # first occurrence from start\n",
        "            # grab up to index as sub-prompt\n",
        "            prompt = text[:idx]\n",
        "            remaining -= idx\n",
        "            # remove from main text\n",
        "            text = text[idx+1:]\n",
        "            # find value for weight \n",
        "            if \" \" in text:\n",
        "                idx = text.index(\" \") # first occurence\n",
        "            else: # no space, read to end\n",
        "                idx = len(text)\n",
        "            if idx != 0:\n",
        "                try:\n",
        "                    weight = float(text[:idx])\n",
        "                except: # couldn't treat as float\n",
        "                    print(f\"Warning: '{text[:idx]}' is not a value, are you missing a space?\")\n",
        "                    weight = 1.0\n",
        "            else: # no value found\n",
        "                weight = 1.0\n",
        "            # remove from main text\n",
        "            remaining -= idx\n",
        "            text = text[idx+1:]\n",
        "            # append the sub-prompt and its weight\n",
        "            prompts.append(prompt)\n",
        "            weights.append(weight)\n",
        "        else: # no : found\n",
        "            if len(text) > 0: # there is still text though\n",
        "                # take remainder as weight 1\n",
        "                prompts.append(text)\n",
        "                weights.append(1.0)\n",
        "            remaining = 0\n",
        "    return prompts, weights\n"
      ],
      "metadata": {
        "id": "XA1NNcxM724U",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown You need to get the model weights yourself and put on Google Drive or this Colab instance\n",
        "checkpoint_model_file = \"/content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "mTjVEfsNlDly",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download weights\n",
        "from huggingface_hub import hf_hub_download\n",
        "from os.path import exists\n",
        "\n",
        "hf_token = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if not hf_token:\n",
        "  raise ValueError(\"Please enter a valid hugging face token or get it here https://huggingface.co/settings/tokens\")\n",
        "\n",
        "if not exists(checkpoint_model_file):\n",
        "  print(\"Downloading weights (locally, if you want to use google drive, put it there yourself)\")\n",
        "  checkpoint_model_file = hf_hub_download(\"CompVis/stable-diffusion-v-1-4-original\", \"sd-v1-4.ckpt\", use_auth_token=hf_token)\n",
        "  print(\"Done!\")\n",
        "else:\n",
        "  print(\"Weights file already exists!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CoB9pcPkoGLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Diffuse!"
      ],
      "metadata": {
        "id": "Bs0rYSMkGqYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Txt2img gradio\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.utils import make_grid\n",
        "from einops import rearrange\n",
        "import os, re\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "from random import randint\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from contextlib import nullcontext\n",
        "from ldm.util import instantiate_from_config\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "import mimetypes\n",
        "mimetypes.init()\n",
        "mimetypes.add_type('application/javascript', '.js')\n",
        "\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "\n",
        "def load_model_from_config(ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    return sd\n",
        "\n",
        "\n",
        "config = \"optimizedSD/v1-inference.yaml\"\n",
        "ckpt = checkpoint_model_file\n",
        "sd = load_model_from_config(f\"{ckpt}\")\n",
        "li, lo = [], []\n",
        "for key, v_ in sd.items():\n",
        "    sp = key.split('.')\n",
        "    if(sp[0]) == 'model':\n",
        "        if('input_blocks' in sp):\n",
        "            li.append(key)\n",
        "        elif('middle_block' in sp):\n",
        "            li.append(key)\n",
        "        elif('time_embed' in sp):\n",
        "            li.append(key)\n",
        "        else:\n",
        "            lo.append(key)\n",
        "for key in li:\n",
        "    sd['model1.' + key[6:]] = sd.pop(key)\n",
        "for key in lo:\n",
        "    sd['model2.' + key[6:]] = sd.pop(key)\n",
        "\n",
        "config = OmegaConf.load(f\"{config}\")\n",
        "\n",
        "model = instantiate_from_config(config.modelUNet)\n",
        "_, _ = model.load_state_dict(sd, strict=False)\n",
        "model.eval()\n",
        "    \n",
        "modelCS = instantiate_from_config(config.modelCondStage)\n",
        "_, _ = modelCS.load_state_dict(sd, strict=False)\n",
        "modelCS.eval()\n",
        "    \n",
        "modelFS = instantiate_from_config(config.modelFirstStage)\n",
        "_, _ = modelFS.load_state_dict(sd, strict=False)\n",
        "modelFS.eval()\n",
        "del sd\n",
        "\n",
        "def generate(prompt,ddim_steps,n_iter, batch_size, Height, Width, scale, ddim_eta,unet_bs, device,seed, outdir,turbo,full_precision,):\n",
        "   \n",
        "    seeds = ''\n",
        "    C = 4\n",
        "    f = 8\n",
        "    start_code = None\n",
        "    model.unet_bs = unet_bs\n",
        "    model.turbo = turbo\n",
        "    model.cdevice = device\n",
        "    modelCS.cond_stage_model.device = device\n",
        "\n",
        "    if device != 'cpu' and full_precision == False:\n",
        "        model.half()\n",
        "        modelCS.half()\n",
        "\n",
        "    tic = time.time()\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    outpath = outdir\n",
        "    sample_path = os.path.join(outpath, '_'.join(re.split(':| ',prompt)))[:150]\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    base_count = len(os.listdir(sample_path))\n",
        "    \n",
        "    if seed == '':\n",
        "        seed = randint(0, 1000000)\n",
        "    seed = int(seed)\n",
        "    seed_everything(seed)\n",
        "\n",
        "    # n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "\n",
        "    if full_precision== False and device != \"cpu\":\n",
        "        precision_scope = autocast\n",
        "    else:\n",
        "        precision_scope = nullcontext\n",
        "\n",
        "    all_samples = []\n",
        "    with torch.no_grad():\n",
        "\n",
        "        all_samples = list()\n",
        "        for _ in trange(n_iter, desc=\"Sampling\"):\n",
        "            for prompts in tqdm(data, desc=\"data\"):\n",
        "                with precision_scope(\"cuda\"):\n",
        "                    modelCS.to(device)\n",
        "                    uc = None\n",
        "                    if scale != 1.0:\n",
        "                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
        "                    if isinstance(prompts, tuple):\n",
        "                        prompts = list(prompts)\n",
        "\n",
        "                    subprompts,weights = split_weighted_subprompts(prompts[0])\n",
        "                    if len(subprompts) > 1:\n",
        "                        c = torch.zeros_like(uc)\n",
        "                        totalWeight = sum(weights)\n",
        "                        # normalize each \"sub prompt\" and add it\n",
        "                        for i in range(len(subprompts)):\n",
        "                            weight = weights[i]\n",
        "                            # if not skip_normalize:\n",
        "                            weight = weight / totalWeight\n",
        "                            c = torch.add(c,modelCS.get_learned_conditioning(subprompts[i]), alpha=weight)\n",
        "                    else:\n",
        "                        c = modelCS.get_learned_conditioning(prompts)\n",
        "\n",
        "                    shape = [C, Height // f, Width // f]\n",
        "\n",
        "                    if device != 'cpu':\n",
        "                        mem = torch.cuda.memory_allocated()/1e6\n",
        "                        modelCS.to(\"cpu\")\n",
        "                        while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                            time.sleep(1)\n",
        "\n",
        "\n",
        "                    samples_ddim = model.sample(S=ddim_steps,\n",
        "                                    conditioning=c,\n",
        "                                    batch_size=batch_size,\n",
        "                                    seed = seed,\n",
        "                                    shape=shape,\n",
        "                                    verbose=False,\n",
        "                                    unconditional_guidance_scale=scale,\n",
        "                                    unconditional_conditioning=uc,\n",
        "                                    eta=ddim_eta,\n",
        "                                    x_T=start_code)\n",
        "\n",
        "                    modelFS.to(device)\n",
        "                    print(\"saving images\")\n",
        "                    for i in range(batch_size):\n",
        "                        \n",
        "                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n",
        "                        x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        all_samples.append(x_sample.to(\"cpu\"))\n",
        "                        x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
        "                        Image.fromarray(x_sample.astype(np.uint8)).save(\n",
        "                            os.path.join(sample_path, \"seed_\" + str(seed) + \"_\" + f\"{base_count:05}.png\"))\n",
        "                        seeds+= str(seed) + ','\n",
        "                        seed+=1\n",
        "                        base_count += 1\n",
        "\n",
        "                    if device != 'cpu':\n",
        "                        mem = torch.cuda.memory_allocated()/1e6\n",
        "                        modelFS.to(\"cpu\")\n",
        "                        while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                            time.sleep(1)\n",
        "                            \n",
        "                    del samples_ddim\n",
        "                    del x_sample\n",
        "                    del x_samples_ddim\n",
        "                    print(\"memory_final = \", torch.cuda.memory_allocated()/1e6)\n",
        "\n",
        "    toc = time.time()\n",
        "\n",
        "    time_taken = (toc-tic)/60.0\n",
        "    grid = torch.cat(all_samples, 0)\n",
        "    grid = make_grid(grid, nrow=n_iter)\n",
        "    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "    \n",
        "    txt = \"Your samples are ready in \" + str(round(time_taken, 3)) + \" minutes and waiting for you here \" + sample_path + \"\\nSeeds used = \" + seeds[:-1]\n",
        "    return Image.fromarray(grid.astype(np.uint8)), txt\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=generate,\n",
        "    inputs=[\"text\",gr.Slider(1, 1000,value=50),gr.Slider(1, 100, step=1),\n",
        "            gr.Slider(1, 100,step=1), gr.Slider(64,4096,value = 512,step=64), \n",
        "            gr.Slider(64,4096,value = 512,step=64),gr.Slider(0,50,value=7.5,step=0.1),\n",
        "            gr.Slider(0,1,step=0.01),gr.Slider(1,2,value = 1,step=1),\n",
        "            gr.Text(value = \"cuda\"),\"text\",gr.Text(value = \"outputs/txt2img-samples\"),\n",
        "            \"checkbox\", \"checkbox\",],\n",
        "    outputs=[\"image\", \"text\"],\n",
        ")\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "erWv8QAEmmqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Img2img gradio\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.utils import make_grid\n",
        "import os, re\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "from random import randint\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from einops import rearrange, repeat\n",
        "from contextlib import nullcontext\n",
        "from ldm.util import instantiate_from_config\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "import mimetypes\n",
        "mimetypes.init()\n",
        "mimetypes.add_type('application/javascript', '.js')\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "\n",
        "def load_model_from_config(ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    return sd\n",
        "\n",
        "def load_img(image, h0, w0):\n",
        "   \n",
        "    image = image.convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h})\")   \n",
        "    if(h0 is not None and w0 is not None):\n",
        "        h, w = h0, w0\n",
        "    \n",
        "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 32\n",
        "\n",
        "    print(f\"New image size ({w}, {h})\")\n",
        "    image = image.resize((w, h), resample = Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "\n",
        "config = \"optimizedSD/v1-inference.yaml\"\n",
        "ckpt = checkpoint_model_file\n",
        "sd = load_model_from_config(f\"{ckpt}\")\n",
        "li, lo = [], []\n",
        "for key, v_ in sd.items():\n",
        "    sp = key.split('.')\n",
        "    if(sp[0]) == 'model':\n",
        "        if('input_blocks' in sp):\n",
        "            li.append(key)\n",
        "        elif('middle_block' in sp):\n",
        "            li.append(key)\n",
        "        elif('time_embed' in sp):\n",
        "            li.append(key)\n",
        "        else:\n",
        "            lo.append(key)\n",
        "for key in li:\n",
        "    sd['model1.' + key[6:]] = sd.pop(key)\n",
        "for key in lo:\n",
        "    sd['model2.' + key[6:]] = sd.pop(key)\n",
        "\n",
        "config = OmegaConf.load(f\"{config}\")\n",
        "\n",
        "model = instantiate_from_config(config.modelUNet)\n",
        "_, _ = model.load_state_dict(sd, strict=False)\n",
        "model.eval()\n",
        "    \n",
        "modelCS = instantiate_from_config(config.modelCondStage)\n",
        "_, _ = modelCS.load_state_dict(sd, strict=False)\n",
        "modelCS.eval()\n",
        "    \n",
        "modelFS = instantiate_from_config(config.modelFirstStage)\n",
        "_, _ = modelFS.load_state_dict(sd, strict=False)\n",
        "modelFS.eval()\n",
        "del sd\n",
        "\n",
        "def generate(image, prompt,strength,ddim_steps,n_iter, batch_size, Height, Width, scale,ddim_eta, unet_bs,device,seed,outdir, turbo, full_precision):\n",
        "\n",
        "    seeds = ''\n",
        "    init_image = load_img(image, Height, Width).to(device)\n",
        "    model.unet_bs = unet_bs\n",
        "    model.turbo = turbo\n",
        "    model.cdevice = device\n",
        "    modelCS.cond_stage_model.device = device\n",
        "\n",
        "    if device != 'cpu' and full_precision == False:\n",
        "        model.half()\n",
        "        modelCS.half()\n",
        "        modelFS.half()\n",
        "        init_image = init_image.half()\n",
        "\n",
        "    tic = time.time()\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    outpath = outdir\n",
        "    sample_path = os.path.join(outpath, '_'.join(re.split(':| ',prompt)))[:150]\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    base_count = len(os.listdir(sample_path))\n",
        "    \n",
        "    if seed == '':\n",
        "        seed = randint(0, 1000000)\n",
        "    seed = int(seed)\n",
        "    seed_everything(seed)\n",
        "\n",
        "    # n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "\n",
        "    modelFS.to(device)\n",
        "\n",
        "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "    init_latent = modelFS.get_first_stage_encoding(modelFS.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    if(device != 'cpu'):\n",
        "        mem = torch.cuda.memory_allocated()/1e6\n",
        "        modelFS.to(\"cpu\")\n",
        "        while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "            time.sleep(1)\n",
        "\n",
        "\n",
        "    assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "    t_enc = int(strength *ddim_steps)\n",
        "    print(f\"target t_enc is {t_enc} steps\")\n",
        "\n",
        "    if full_precision== False and device != \"cpu\":\n",
        "        precision_scope = autocast\n",
        "    else:\n",
        "        precision_scope = nullcontext\n",
        "\n",
        "    all_samples = []\n",
        "    with torch.no_grad():\n",
        "        all_samples = list()\n",
        "        for _ in trange(n_iter, desc=\"Sampling\"):\n",
        "            for prompts in tqdm(data, desc=\"data\"):\n",
        "                with precision_scope(\"cuda\"):\n",
        "                    modelCS.to(device)\n",
        "                    uc = None\n",
        "                    if scale != 1.0:\n",
        "                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
        "                    if isinstance(prompts, tuple):\n",
        "                        prompts = list(prompts)\n",
        "\n",
        "                    subprompts,weights = split_weighted_subprompts(prompts[0])\n",
        "                    if len(subprompts) > 1:\n",
        "                        c = torch.zeros_like(uc)\n",
        "                        totalWeight = sum(weights)\n",
        "                        # normalize each \"sub prompt\" and add it\n",
        "                        for i in range(len(subprompts)):\n",
        "                            weight = weights[i]\n",
        "                            # if not skip_normalize:\n",
        "                            weight = weight / totalWeight\n",
        "                            c = torch.add(c,modelCS.get_learned_conditioning(subprompts[i]), alpha=weight)\n",
        "                    else:\n",
        "                        c = modelCS.get_learned_conditioning(prompts)\n",
        "                    \n",
        "                    c = modelCS.get_learned_conditioning(prompts)\n",
        "                    if(device != 'cpu'):\n",
        "                        mem = torch.cuda.memory_allocated()/1e6\n",
        "                        modelCS.to(\"cpu\")\n",
        "                        while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                            time.sleep(1)\n",
        "\n",
        "                    # encode (scaled latent)\n",
        "                    z_enc = model.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device), seed,ddim_eta,ddim_steps)\n",
        "                    # decode it\n",
        "                    samples_ddim = model.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
        "                                                    unconditional_conditioning=uc,)\n",
        "\n",
        "                    modelFS.to(device)\n",
        "                    print(\"saving images\")\n",
        "                    for i in range(batch_size):\n",
        "                        \n",
        "                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n",
        "                        x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        all_samples.append(x_sample.to(\"cpu\"))\n",
        "                        x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
        "                        Image.fromarray(x_sample.astype(np.uint8)).save(\n",
        "                            os.path.join(sample_path, \"seed_\" + str(seed) + \"_\" + f\"{base_count:05}.png\"))\n",
        "                        seeds+= str(seed) + ','\n",
        "                        seed+=1\n",
        "                        base_count += 1\n",
        "\n",
        "\n",
        "                    if(device != 'cpu'):\n",
        "                        mem = torch.cuda.memory_allocated()/1e6\n",
        "                        modelFS.to(\"cpu\")\n",
        "                        while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                            time.sleep(1)\n",
        "\n",
        "                    del samples_ddim\n",
        "                    del x_sample\n",
        "                    del x_samples_ddim\n",
        "                    print(\"memory_final = \", torch.cuda.memory_allocated()/1e6)\n",
        "\n",
        "    toc = time.time()\n",
        "\n",
        "    time_taken = (toc-tic)/60.0\n",
        "    grid = torch.cat(all_samples, 0)\n",
        "    grid = make_grid(grid, nrow=n_iter)\n",
        "    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "    \n",
        "    txt = \"Your samples are ready in \" + str(round(time_taken, 3)) + \" minutes and waiting for you here \\n\" + sample_path + \"\\nSeeds used = \" + seeds[:-1]\n",
        "    return Image.fromarray(grid.astype(np.uint8)), txt\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=generate,\n",
        "    inputs=[gr.Image(tool=\"editor\", type=\"pil\"),\"text\",gr.Slider(0, 1,value=0.75),\n",
        "            gr.Slider(1, 1000,value=50),gr.Slider(1, 100, step=1), gr.Slider(1, 100,step=1),\n",
        "            gr.Slider(64,4096,value = 512,step=64), gr.Slider(64,4096,value = 512,step=64), \n",
        "            gr.Slider(0,50,value=7.5,step=0.1),gr.Slider(0,1,step=0.01),\n",
        "            gr.Slider(1,2,value = 1,step=1),gr.Text(value = \"cuda\"), \"text\",\n",
        "            gr.Text(value = \"outputs/img2img-samples\"),\"checkbox\", \"checkbox\",],\n",
        "    outputs=[\"image\", \"text\"],\n",
        ")\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Xd5nVVgum59F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "o8hwXeJhIfJl",
        "QEX4emjII54P"
      ],
      "machine_shape": "hm",
      "name": "Stable Diffusion Lite",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}